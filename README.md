使用随机过程数学方法对深度学习模型进行优化，一方面是优化随机梯度下降，另一方面是优化丢弃率

在本研究中，我们提出了一种基于随机过程优化SGD的方法，是通过引入马尔可夫链动态调
整学习率，以增强模型的适应性和训练效果。具体而言，我们将学习率视作一个马尔可夫过程，根
据当前的损失值和梯度信息调整学习率，使模型能够在训练早期阶段使用较高学习率，以便快速探
索参数空间，而在后期阶段逐渐减小学习率以精细调整。换言之：若损失下降速度缓慢，则增加学
习率；若下降速度较快，则减小学习率。马尔可夫链模型定义为[3]：ηt+1=f(ηt,L(θt),∇θL(θt))
其中，ηt+1 表示下一个训练步的学习率，函数f是基于马尔可夫过程的学习率更新策略。我
们通过设置一系列状态（如高、低、适中）来模拟学习率变化，确保学习率能够动态响应模型的
训练状态，以提高训练效率和收敛性能。该策略在模型训练过程中能够动态适应不同训练阶段的
需求，提升训练效率并降低损失。


为了克服传统 Dropout 的局限性，本研究提出了一种基于随机过程的创新方法——贝叶斯
Dropout。该方法通过动态调整丢弃概率来提升模型的泛化能力，具体包括以下几个方面：
1. 贝叶斯框架：
贝叶斯Dropout 利用贝叶斯推理的思想，将丢弃概率视作随机变量，通过后验分布对丢弃
率进行建模。在训练过程中，模型不仅学习参数，还同时学习丢弃概率的分布。
2. 动态调整丢弃概率：
引入随机过程来根据当前模型性能和训练阶段动态调整丢弃概率。例如，在模型表现较差时，
可以增加丢弃率以增强模型的鲁棒性；而在模型表现良好时，则降低丢弃率，以保留更多的
特征信息。这种自适应的丢弃策略使得模型在不同阶段都能够最大化泛化能力。
3. 提升模型泛化能力：
通过随机过程动态调整丢弃概率，贝叶斯Dropout 可以有效减少模型的过拟合，增强在新
数据上的表现。此外，模型在训练过程中能够更灵活地适应数据的变化，提升学习效果。
